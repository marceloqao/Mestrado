\mychapter{Proposta}{chp:Proposta}
\lhead{PROPOSTA}

% Breve resumo do capítulo.
  \lettrine{E}{ste} capítulo tem como objetivo apresentar a proposta bem como os materiais e métodos utilizados na realização do trabalho.
  
\section{Fundamentação Teórica}

Trataremos a sequência de saída de um gerador de números pseudoaleatórios como uma série temporal.
Procuraremos por estruturas indesejadas em um gerador ideal utilizando ferramentas de Teoria da Informação.

Há diversas abordagens para a análise de séries temporais, sendo que uma das mais importantes surge a partir do trabalho de \citet{PermutationEntropyBandtPompe}.

Essa abordagem consiste em transformar a série de entrada em uma sequência de símbolos.
Feito isso, computa-se o histograma de proporções dos símbolos, e calculam-se descritores a partir dele.

A hipótese subjacente a toda análise de séries temporais é que os dados observados são o resultado da operação de um sistema causal, possivelmente sujeito a ruído observacional.
Esse sistema, ou dinâmica, é responsável pela criação de padrões através de cuja observação deseja-se inferir a respeito da dinâmica.
 
A análise de séries temporais é um ramo clássico da Estatística~\cite{BrockwellDavis91} que se divide, tipicamente, na análise nos domínios do tempo e da frequência.
Ambas abordagens empregam diretamente os valores observados e, portanto, são suscetíveis ao efeito danoso de diversos tipos de contaminação.
Uma forma de tornar as análises mais imunes a contaminação é através de técnicas robustas~\cite{BustosFraiman1984}.
Outra, mais moderna, é pelo uso de métodos não-paramétricos.
 
Há diversas ferramentas que auxiliam na análise clássica de séries temporais; na data de redação deste trabalho havia \num{238} bibliotecas para essa finalidade (ver \url{https://cran.r-project.org/web/views/TimeSeries.html}).
Para essa mesma plataforma, apenas três delas bibliotecas trabalham exclusivamente com técnicas não paramétricas.

Seja a série temporal $\bm x = (x_1, x_2, \dots, x_n)$.
Ao invés de analisarmos os valores, transformaremos grupos de $N$ valores (não necessariamente adjacentes) em padrões ordinais, e analisaremos a sua distribuição de frequência.
Por exemplo e sem perda de generalidade, com $N=3$ e para qualquer $i$ viável,
se $x_i<x_{i+1}<x_{i+2}$ atribuiremos a esta tripla o padrão $\pi_0$;
caso $x_i>x_{i+1}>x_{i+2}$ o padrão será $\pi_1$ e assim por diante.
Com isso, há $N!$ possíveis padrões.
Esta é conhecida como \textit{simbolização de Bandt \& Pompe}~\cite{PermutationEntropyBandtPompe}.
Forma-se, então, um histograma e, a partir dele, extraem-se quantificadores como, por exemplo, entropia, distância estocástica a uma distribuição de equilíbrio, e complexidade estatística.
 
Esta simbolização é muito resistente a vários tipos de contaminação, por exemplo, o padrão $\pi_0$ não será alterado para qualquer $k>1$ que afete de forma multiplicativa $x_{i+2}$.
Ainda que o padrão seja alterado, por exemplo se $k=-1$, a mudança será local e afetará, no máximo, $N$ padrões.
 
A análise da dinâmica subjacente a uma série temporal utilizando a simbolização de Bandt \& Pompe tem sido usada com sucesso em diversas áreas como, por exemplo, 
a discriminação entre fenómenos estocásticos e caóticos~\cite{DistinguishingNoiseFromChaos}, 
a identificação de padrões de comportamento em redes veiculares~\cite{CharacterizationVehicleBehaviorInformationTheory},
a classificação e verificação de assinaturas \textit{online}~\cite{ClassificationVerificationOnlineHandwrittenSignatures},
na análise da robustez de redes~\cite{InformationTheoryPerspectiveNetworkRobustness},
a classificação de padrões de consumo de energia elétrica~\cite{CharacterizationElectricLoadInformationTheoryQuantifiers}.

o processo de simbolização, também conhecido como particionamento de dados, representa o procedimento de distribuição dos elementos em conjuntos de símbolos capazes de fornecer a sua informação causal. 
De acordo com a abordagem de Bandt e Pompe, substituímos a série por sequências de postos, obtidos pela análise desta ao longo do tempo.

Dada uma série temporal a tempo discreto $X = {x_t:1\leq t\leq M}$ de comprimento $N$, uma dimensão $D$ e um tempo de atraso (\textit{delay}) $\tau$, o particionamento é efetuado por meio da reorganização do sistema em conjuntos seguindo os passos:
\begin{itemize}
%%% ACF Reescreva usando modo matemático sempre que necessário, e respeitando a notação
%%% Use o pacote "listings"
	\item \textbf{Composição dos grupos:} Os conjuntos, ou palavras, de comprimento $D$ e \textit{delay} $\tau$ são definidas por um segmento da série: 
	 $$ (x_{t+1)},x_{t+\tau},\ldots, x{t+D\tau}.$$ 

	\item \textbf{Formação dos padrões: } Cada palavra é então relacionada a um padrão ordinal $\pi_j$ de ordem $D$, isto é, um elemento indexado univocamente por
	$$ j\in\{1, 2,\ldots, D-1, D\}. $$
\end{itemize}

Há várias formas de atribuir palavras a símbolos; neste trabalho utilizaremos a atribuição lexicográfica, isto é, se os valores da palavra $(x_{t+1)},x_{t+\tau},\ldots, x{t+D\tau}$ são tais que, ordenados, eles têm índices crescentes $b_1,b_2,\dots,b_D$, então o padrão correspondente será $\pi=b_1b_2\dots b_D$.

Calcula-se, então, o histrograma de proporções $\mathcal{H} =(p_1,\dots,p_D)$ dos padrões observados:
$$
p_j = \frac{1}{?} \# \{
\textbf{padroões } \pi_j \text{ observados}.
\}
$$

O seguinte passo consiste em calcular descritores a partir desse histograma.

\subsection{Descritores}

Os trabalhos já citados utilizam dois descritores: a Entropia de Shannon e a Complexidade Estatística da série.



\section{Regiões de Confiança no Plano Entropia-Complexidade}

Seja $\bm X = (X_1, X_n)$ uma amostra a respeito da qual temos uma conjectura que queremos verificar.
Essa conjectura é a respeito dos parâmetros que caracterizam a distribuição da amostra, ou a respeito de parâmetros que caracterizam a distribuição de atributos relacionados à distribuição da amostra.
Chamaremos ``hipótese nula'' àquela que convimos em não rejeitar a não ser que obtenhamos suficiente evidência para isso; a denotaremos $H_0$.
Por vezes precisaremos da ``hipótese alternativa'', que denotaremos $H_1$.

Classicamente, um teste de hipótese se baseia em uma estatística de teste $T$ que depende exclusivamente da amostra $\bm X$, isto é, $T(\bm X)$, e é construída de tal forma que adota valores ``pequenos'' sob $H_0$ e ``cresce'' conforme ``se afasta'' de $H_0$.
Idealmente, conhecemos a distribuição de $T$ sob a hipótese nula e, com isso, somos capazes de aferir a probabilidade de observarmos valores ``grandes'' mesmo sob $H_0$.
Definimos, assim, o $p$-valor do teste baseado em $T(\bm X)$ para o valor observado $\eta$ como $\Pr_{H_0}(T(\bm X) \geq \eta)$.
O procedimento básico consiste em rejeitar a hipótese nula ao nível de significância $100(1-\alpha)\%$ se o $p$-valor for inferior a $\alpha$, e em não rejeitá-la caso contrário.
Mais modernamente, não se fala em ``rejeição'', reporta-se o $p$-valor, deixando a decisão para o leitor. Desta forma, chamaremos de ``valor crítico'' o conjunto de valores, definidos pelo leitor tais que, quando assumidos pela estatística de teste $T$ levam à rejeição da hipótese, nula $H_0$. 
% De forma análoga, utilizaremos o termo ``nível de significância'' como a probabilidade máxima de $\alpha$ que nos leve a não rejeitar $H_0$ e acima da qual rejeita-se $H_0$, ou seja a probabilidade de rejeitar $H_0$ quando a mesma é verdadeira.

Diversos testes foram desenvolvidos ao longo do tempo com o objetivo de testar a aleatoriedade de sequências de números.
Como ``aleatoriedade'' é uma noção vaga, cada teste procura identificar uma ou algumas falhas dos dados.
Por esse motivo é que há classes ou conjuntos de testes que, aplicados de forma criteriosa, permitem aferir o quanto uma sequência se afasta da hipótese de aleatoriedade.
A partir dessa análise, pode se ter uma idea do comportamento global do gerador que a produziu.

Sob a conjectura de aleatoriedade, cada estatística de teste $T$ tem uma distribuição, que pode ser determinada ou de forma exata ou de forma aproximada, mas sempre precisa ser conhecida.
Com ela é possível informar o $p$-valor de uma determinada amostra.
	Em relação a fundamentação teórica, utilizou-se como principal fonte de pesquisa a área de indexação de periódicos científicos ISI \emph{Web of Knowledge}, onde foram obtidas a grande maioria das referências, usando como parâmetros o fator de impacto dos periódicos pesquisados, a quantidade de citações de cada publicação, o grau de relevância para o tema pesquisado e o nível de produtividade (fator-H) dos autores envolvidos. O apoio em livros, surveys, lecture notes e ferramentas complementares de busca, como o \emph{google acadêmico} foram utilizadas para complementar esta pesquisa.

 \section{Artefatos}Para organizar, catalogar e facilitar a consulta a todo material obtido, as referências foram gerenciadas com a ferramenta \emph{Mendeley}. Um gerenciador de referências bibliográficas multiplataforma gratuito que permite organizar de maneira centralizada vários vínculos entre as referências utilizadas, bem como visualizar e anotar as mesmas dentro da própria ferramenta. Quanto à editoração eletrônica do trabalho, fez-se uso da plataforma \LaTeX, com editor de textos \emph{Kile} de código aberto. Este trabalho foi desenvolvido num equipamento com as seguintes configurações:

\begin{center}
\begin{tabular}{c||c}
\hline
\textbf{Arquitetura} & \texttt{Intel i7 $64$ bits}\\
\hline
\textbf{S.O.} & Linux Mint 17.1 - kernel $3.13.0-43-generic$\\
\hline
\textbf{Editor} & Kile versão $2.1.3$ e \LaTeX texlive $2013.20140215-1$\\
\hline
\end{tabular}
\end{center}

  Do ponto de vista técnico deste trabalho, com ênfase em Geradores de Números Pseudoaleatórios, é utilizada a plataforma de análise estatística R . Esta plataforma foi desenvolvida originalmente por Ross Ihaka e Robert Gentleman, com o intuito de ser uma linguagem de código aberto voltada para a análise estatística e, consequentemente, a precisão numérica com fortes características funcionais \citep{Rmanual}. Por este motivo, ela será utilizada para a geração e manipulação das sequências pseudoaleatórias, análise dos dados e geração dos gráficos desse trabalho. A precisão numérica desta ferramenta, sendo aferida por \citet{Almiron:09}, é adequada para essa abordagem.

  As ferramentas utilizadas no desenvolvimento deste trabalho, são preferencialmente multiplataforma e código aberto com licença de uso \emph{GNU General Public License}~(GPL).

  Todos esses aplicativos, métodos e informações obtidas, forneceram grandes contribuições para o traçado da linha mestra deste trabalho, indicando que o mesmo está na fronteira do conhecimento produzindo um estado da arte fidedigno aos temas e ferramentas adotadas para norteá-lo.

\begin{center}
	\fbox{
	\colorbox[RGB]{227, 227, 227}{
	\parbox[t]{.8\linewidth}
		{Neste capítulo tratamos da metodologia utilizada do desenvolvimento do trabalho, no capítulo seguinte analisaremos os impactos esperados com a realização do mesmo.}} }
\end{center}
